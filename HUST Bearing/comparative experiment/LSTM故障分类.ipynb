{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据和训练数据分组，数据分batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "31\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from joblib import dump, load\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 参数与配置\n",
    "torch.manual_seed(100)  # 设置随机种子，以使实验结果具有可重复性\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 有GPU先用GPU训练\n",
    "\n",
    "# 加载数据集\n",
    "def dataloader(batch_size, workers=2):\n",
    "    # 训练集\n",
    "    train_xdata = load('train_xdata')\n",
    "    train_ylabel = load('train_ylabel')\n",
    "    # 验证集\n",
    "    val_xdata = load('val_xdata')\n",
    "    val_ylabel = load('val_ylabel')\n",
    "    # 测试集\n",
    "    test_xdata = load('test_xdata')\n",
    "    test_ylabel = load('test_ylabel')\n",
    "\n",
    "    # 加载数据\n",
    "    train_loader = Data.DataLoader(dataset=Data.TensorDataset(train_xdata, train_ylabel),\n",
    "                                   batch_size=batch_size, shuffle=True, num_workers=workers, drop_last=True)\n",
    "    val_loader = Data.DataLoader(dataset=Data.TensorDataset(val_xdata, val_ylabel),\n",
    "                                 batch_size=batch_size, shuffle=True, num_workers=workers, drop_last=True)\n",
    "    test_loader = Data.DataLoader(dataset=Data.TensorDataset(test_xdata, test_ylabel),\n",
    "                                  batch_size=batch_size, shuffle=True, num_workers=workers, drop_last=True)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "batch_size = 64\n",
    "# 加载数据\n",
    "train_loader, val_loader, test_loader = dataloader(batch_size)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from joblib import dump, load\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMclassifier(nn.Module):\n",
    "    def __init__(self, input_dim,hidden_layer_sizes,output_dim, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        LSTM 分类任务  params:\n",
    "        batch_size       : 批次量大小\n",
    "        input_dim        : 输入数据的维度\n",
    "        hidden_layer_size:隐层的数目和维度\n",
    "        output_dim       : 输出的维度\n",
    "        dropout_rate     : 随机丢弃神经元的概率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 批次量大小\n",
    "        # lstm层数\n",
    "        self.num_layers = len(hidden_layer_sizes)\n",
    "        self.lstm_layers = nn.ModuleList()  # 用于保存LSTM层的列表\n",
    "\n",
    "        # 定义第一层LSTM   \n",
    "        self.lstm_layers.append(nn.LSTM(input_dim, hidden_layer_sizes[0], batch_first=True))\n",
    "        \n",
    "        # 定义后续的LSTM层\n",
    "        for i in range(1, self.num_layers):\n",
    "                self.lstm_layers.append(nn.LSTM(hidden_layer_sizes[i-1], hidden_layer_sizes[i], batch_first=True))\n",
    "                \n",
    "        # 定义全连接层\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_layer_sizes[-1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "       \n",
    "    def forward(self, input_seq):  # torch.Size([16, 512])\n",
    "        # 前向传播的过程是输入->LSTM层->全连接层->输出\n",
    "        # 在观察查看LSTM输入的维度，LSTM的第一个输入input_size维度是(batch, seq_length, H_in) batch是batch size , seq_length是序列长度，H_in是输入维度，也就是变量个数\n",
    "        # LSTM的第二个输入是一个元组，包含了h0,c0两个元素，这两个元素的维度都是（D∗num_layers,N,H_out)，\n",
    "        # D=1表示单向网络，num_layers表示多少个LSTM层叠加，N是batch size，H_out表示隐层神经元个数\n",
    "\n",
    "        # 数据预处理\n",
    "        #改变输入形状，适应网络输入[batch, seq_length, H_in]\n",
    "        # 注意：这里是 把数据进行了堆叠 把一个1*1024 的序列 进行 划分堆叠成形状为 32 * 32， 就使输入序列的长度降下来了\n",
    "        # 序列如果 以1024 长度输入进网络，LSTM 容易发生 梯度消失或者爆炸，不容易训练出好结果\n",
    "        # 当然， 还可以 堆叠 为其他形状的矩阵\n",
    "        batch_size = input_seq.size(0)\n",
    "        input_seq = input_seq.view(batch_size, 32, 32) \n",
    "        lstm_out = input_seq\n",
    "        \n",
    "        for lstm in self.lstm_layers:\n",
    "            lstm_out, _= lstm(lstm_out)  ## 进行一次LSTM层的前向传播\n",
    "        # print(lstm_out.size())  # torch.Size([64, 32, 32])\n",
    "        out = self.classifier(lstm_out[:, -1, :]) # torch.Size([64, 10]  # 仅使用最后一个时间步的输出 \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入数据的形状是\n",
    " 1. print(seq.size(), labels.size()) \n",
    " 2. torch.Size([64, 1, 1024])    torch.Size([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32768\n",
      "262144\n",
      "  1024\n",
      "  1024\n",
      "131072\n",
      " 65536\n",
      "   512\n",
      "   512\n",
      " 32768\n",
      " 16384\n",
      "   256\n",
      "   256\n",
      "  4096\n",
      "    64\n",
      "   320\n",
      "     5\n",
      "______\n",
      "548741\n"
     ]
    }
   ],
   "source": [
    "# 定义模型参数\n",
    "input_dim = 32   # 输入维度为一维信号序列堆叠为  32 * 32\n",
    "hidden_layer_sizes = [256, 128, 64]  # LSTM 层数， 每层 神经元个数\n",
    "output_dim = 5\n",
    "\n",
    "model = LSTMclassifier(input_dim, hidden_layer_sizes, output_dim)  \n",
    "# 定义损失函数和优化函数 \n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(reduction='sum')  # loss\n",
    "learn_rate = 0.0003\n",
    "optimizer = torch.optim.Adam(model.parameters(), learn_rate)  # 优化器\n",
    "\n",
    "# 看下这个网络结构总共有多少个参数\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMclassifier(\n",
      "  (lstm_layers): ModuleList(\n",
      "    (0): LSTM(32, 256, batch_first=True)\n",
      "    (1): LSTM(256, 128, batch_first=True)\n",
      "    (2): LSTM(128, 64, batch_first=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意调整参数，\n",
    "1. 可以适当增加 LSTM层数 和每层神经元个数，微调学习率；\n",
    "2. 增加更多的 epochs,  （注意防止过拟合）\n",
    "3. 但是通过对比实验来看，LSTM对于这种长须列信号的分类能力不如CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 train_Loss: 6.44217057 train_Accuracy:0.7905\n",
      "Epoch:  1 val_Loss:6.44260088,  validate_Acc:0.7560\n",
      "Epoch:  2 train_Loss: 6.44034817 train_Accuracy:0.7703\n",
      "Epoch:  2 val_Loss:6.44135217,  validate_Acc:0.7540\n",
      "Epoch:  3 train_Loss: 6.43861442 train_Accuracy:0.8148\n",
      "Epoch:  3 val_Loss:6.43923618,  validate_Acc:0.8165\n",
      "Epoch:  4 train_Loss: 6.44017863 train_Accuracy:0.8102\n",
      "Epoch:  4 val_Loss:6.44013340,  validate_Acc:0.7661\n",
      "Epoch:  5 train_Loss: 6.43878283 train_Accuracy:0.7889\n",
      "Epoch:  5 val_Loss:6.43890736,  validate_Acc:0.7621\n",
      "Epoch:  6 train_Loss: 6.43618598 train_Accuracy:0.8283\n",
      "Epoch:  6 val_Loss:6.43667543,  validate_Acc:0.7722\n",
      "Epoch:  7 train_Loss: 6.43555559 train_Accuracy:0.8074\n",
      "Epoch:  7 val_Loss:6.43156141,  validate_Acc:0.8468\n",
      "Epoch:  8 train_Loss: 6.44267569 train_Accuracy:0.8029\n",
      "Epoch:  8 val_Loss:6.43751349,  validate_Acc:0.7782\n",
      "Epoch:  9 train_Loss: 6.43528592 train_Accuracy:0.7917\n",
      "Epoch:  9 val_Loss:6.43600138,  validate_Acc:0.7782\n",
      "Epoch: 10 train_Loss: 6.43320814 train_Accuracy:0.8153\n",
      "Epoch: 10 val_Loss:6.43101028,  validate_Acc:0.7782\n",
      "Epoch: 11 train_Loss: 6.43216370 train_Accuracy:0.8086\n",
      "Epoch: 11 val_Loss:6.42997554,  validate_Acc:0.7742\n",
      "Epoch: 12 train_Loss: 6.43025610 train_Accuracy:0.7821\n",
      "Epoch: 12 val_Loss:6.42754112,  validate_Acc:0.7782\n",
      "Epoch: 13 train_Loss: 6.42773059 train_Accuracy:0.8164\n",
      "Epoch: 13 val_Loss:6.42765305,  validate_Acc:0.7742\n",
      "Epoch: 14 train_Loss: 6.43685871 train_Accuracy:0.7939\n",
      "Epoch: 14 val_Loss:6.43214041,  validate_Acc:0.7742\n",
      "Epoch: 15 train_Loss: 6.43229566 train_Accuracy:0.7776\n",
      "Epoch: 15 val_Loss:6.42844515,  validate_Acc:0.7601\n",
      "Epoch: 16 train_Loss: 6.43075477 train_Accuracy:0.7725\n",
      "Epoch: 16 val_Loss:6.42656935,  validate_Acc:0.7923\n",
      "Epoch: 17 train_Loss: 6.42695202 train_Accuracy:0.8069\n",
      "Epoch: 17 val_Loss:6.42467011,  validate_Acc:0.8085\n",
      "Epoch: 18 train_Loss: 6.42720444 train_Accuracy:0.7883\n",
      "Epoch: 18 val_Loss:6.42663968,  validate_Acc:0.7621\n",
      "Epoch: 19 train_Loss: 6.42742145 train_Accuracy:0.7776\n",
      "Epoch: 19 val_Loss:6.42465439,  validate_Acc:0.7742\n",
      "Epoch: 20 train_Loss: 6.42724240 train_Accuracy:0.7843\n",
      "Epoch: 20 val_Loss:6.42657234,  validate_Acc:0.7702\n",
      "Epoch: 21 train_Loss: 6.42827607 train_Accuracy:0.7883\n",
      "Epoch: 21 val_Loss:6.42609313,  validate_Acc:0.7601\n",
      "Epoch: 22 train_Loss: 6.42664722 train_Accuracy:0.8102\n",
      "Epoch: 22 val_Loss:6.42711389,  validate_Acc:0.7702\n",
      "Epoch: 23 train_Loss: 6.42471465 train_Accuracy:0.8193\n",
      "Epoch: 23 val_Loss:6.42423158,  validate_Acc:0.7762\n",
      "Epoch: 24 train_Loss: 6.42780884 train_Accuracy:0.7956\n",
      "Epoch: 24 val_Loss:6.42313643,  validate_Acc:0.7661\n",
      "Epoch: 25 train_Loss: 6.42677898 train_Accuracy:0.8069\n",
      "Epoch: 25 val_Loss:6.42503352,  validate_Acc:0.7621\n",
      "Epoch: 26 train_Loss: 6.42656898 train_Accuracy:0.7995\n",
      "Epoch: 26 val_Loss:6.42313906,  validate_Acc:0.7802\n",
      "Epoch: 27 train_Loss: 6.42694947 train_Accuracy:0.7950\n",
      "Epoch: 27 val_Loss:6.42355027,  validate_Acc:0.7702\n",
      "Epoch: 28 train_Loss: 6.42471327 train_Accuracy:0.8029\n",
      "Epoch: 28 val_Loss:6.42317107,  validate_Acc:0.7722\n",
      "Epoch: 29 train_Loss: 6.42676715 train_Accuracy:0.8007\n",
      "Epoch: 29 val_Loss:6.42390885,  validate_Acc:0.7681\n",
      "Epoch: 30 train_Loss: 6.42544814 train_Accuracy:0.7967\n",
      "Epoch: 30 val_Loss:6.42339889,  validate_Acc:0.7742\n",
      "Epoch: 31 train_Loss: 6.42587209 train_Accuracy:0.8193\n",
      "Epoch: 31 val_Loss:6.42408323,  validate_Acc:0.7702\n",
      "Epoch: 32 train_Loss: 6.42570905 train_Accuracy:0.8102\n",
      "Epoch: 32 val_Loss:6.42388105,  validate_Acc:0.7681\n",
      "Epoch: 33 train_Loss: 6.42625102 train_Accuracy:0.7945\n",
      "Epoch: 33 val_Loss:6.42390142,  validate_Acc:0.7782\n",
      "Epoch: 34 train_Loss: 6.42470220 train_Accuracy:0.7967\n",
      "Epoch: 34 val_Loss:6.42320690,  validate_Acc:0.7681\n",
      "Epoch: 35 train_Loss: 6.42629294 train_Accuracy:0.7973\n",
      "Epoch: 35 val_Loss:6.42411872,  validate_Acc:0.7742\n",
      "Epoch: 36 train_Loss: 6.42576688 train_Accuracy:0.7990\n",
      "Epoch: 36 val_Loss:6.42710366,  validate_Acc:0.7641\n",
      "Epoch: 37 train_Loss: 6.42571287 train_Accuracy:0.8350\n",
      "Epoch: 37 val_Loss:6.42380170,  validate_Acc:0.7762\n",
      "Epoch: 38 train_Loss: 6.42499227 train_Accuracy:0.7832\n",
      "Epoch: 38 val_Loss:6.42338519,  validate_Acc:0.7601\n",
      "Epoch: 39 train_Loss: 6.42503320 train_Accuracy:0.8164\n",
      "Epoch: 39 val_Loss:6.42262356,  validate_Acc:0.7702\n",
      "Epoch: 40 train_Loss: 6.42651923 train_Accuracy:0.8384\n",
      "Epoch: 40 val_Loss:6.42258659,  validate_Acc:0.7601\n",
      "Epoch: 41 train_Loss: 6.42546203 train_Accuracy:0.8215\n",
      "Epoch: 41 val_Loss:6.42271068,  validate_Acc:0.7702\n",
      "Epoch: 42 train_Loss: 6.42450381 train_Accuracy:0.8226\n",
      "Epoch: 42 val_Loss:6.42354282,  validate_Acc:0.7722\n",
      "Epoch: 43 train_Loss: 6.42621030 train_Accuracy:0.7917\n",
      "Epoch: 43 val_Loss:6.42398662,  validate_Acc:0.7681\n",
      "Epoch: 44 train_Loss: 6.42522076 train_Accuracy:0.8069\n",
      "Epoch: 44 val_Loss:6.42302315,  validate_Acc:0.7782\n",
      "Epoch: 45 train_Loss: 6.42592312 train_Accuracy:0.8086\n",
      "Epoch: 45 val_Loss:6.42541081,  validate_Acc:0.7782\n",
      "Epoch: 46 train_Loss: 6.42472740 train_Accuracy:0.8131\n",
      "Epoch: 46 val_Loss:6.42347307,  validate_Acc:0.7681\n",
      "Epoch: 47 train_Loss: 6.42617620 train_Accuracy:0.8012\n",
      "Epoch: 47 val_Loss:6.42331611,  validate_Acc:0.7762\n",
      "Epoch: 48 train_Loss: 6.42537823 train_Accuracy:0.8215\n",
      "Epoch: 48 val_Loss:6.42343653,  validate_Acc:0.7601\n",
      "Epoch: 49 train_Loss: 6.42635279 train_Accuracy:0.8176\n",
      "Epoch: 49 val_Loss:6.42318972,  validate_Acc:0.7581\n",
      "Epoch: 50 train_Loss: 6.42413428 train_Accuracy:0.8052\n",
      "Epoch: 50 val_Loss:6.42294899,  validate_Acc:0.7742\n",
      "\n",
      "Duration: 205 seconds\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc(\"font\", family='Microsoft YaHei')\n",
    "\n",
    "def model_train(batch_size, epochs, model, optimizer, loss_function, train_loader, val_loader):\n",
    "    model = model.to(device)\n",
    "    # 样本长度\n",
    "    train_size = len(train_loader) * batch_size\n",
    "    val_size = len(val_loader) * batch_size\n",
    "\n",
    "    # 最高准确率  最佳模型\n",
    "    best_accuracy = 0.0\n",
    "    best_model = model\n",
    "\n",
    "    train_loss = []     # 记录在训练集上每个epoch的loss的变化情况\n",
    "    train_acc = []    # 记录在训练集上每个epoch的准确率的变化情况\n",
    "    validate_acc = []\n",
    "    validate_loss = []\n",
    "\n",
    "    # 计算模型运行时间\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        # 训练\n",
    "        model.train()\n",
    "\n",
    "        loss_epoch = 0.    #保存当前epoch的loss和\n",
    "        correct_epoch = 0  #保存当前epoch的正确个数和\n",
    "        for seq, labels in train_loader: \n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            # print(seq.size(), labels.size()) torch.Size([32, 7, 1024]) torch.Size([32])\n",
    "            # 每次更新参数前都梯度归零和初始化\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            y_pred = model(seq)  #   torch.Size([16, 10])\n",
    "            # 对模型输出进行softmax操作，得到概率分布\n",
    "            probabilities = F.softmax(y_pred, dim=1)\n",
    "            # 得到预测的类别\n",
    "            predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "            # 与真实标签进行比较，计算预测正确的样本数量  # 计算当前batch预测正确个数\n",
    "            correct_epoch += (predicted_labels == labels).sum().item()\n",
    "            # 损失计算\n",
    "            loss = loss_function(y_pred, labels)\n",
    "            loss_epoch += loss.item()\n",
    "            # 反向传播和参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #     break\n",
    "        # break\n",
    "        # 计算准确率\n",
    "        train_Accuracy  = correct_epoch/train_size \n",
    "        train_loss.append(loss_epoch/train_size)\n",
    "        train_acc.append(train_Accuracy)\n",
    "        print(f'Epoch: {epoch+1:2} train_Loss: {loss_epoch/train_size:10.8f} train_Accuracy:{train_Accuracy:4.4f}')\n",
    "        # 每一个epoch结束后，在验证集上验证实验结果。\n",
    "        with torch.no_grad():\n",
    "            # 将模型设置为评估模式\n",
    "            model.eval()\n",
    "            loss_validate = 0.\n",
    "            correct_validate = 0\n",
    "            for data, label in val_loader:\n",
    "                data, label = data.to(device), label.to(device)\n",
    "                pre = model(data)\n",
    "                # 对模型输出进行softmax操作，得到概率分布\n",
    "                probabilities = F.softmax(pre, dim=1)\n",
    "                # 得到预测的类别\n",
    "                predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "                # 与真实标签进行比较，计算预测正确的样本数量  # 计算当前batch预测正确个数\n",
    "                correct_validate += (predicted_labels == label).sum().item()\n",
    "                loss = loss_function(pre, label)\n",
    "                loss_validate += loss.item()\n",
    "            # print(f'validate_sum:{loss_validate},  validate_Acc:{correct_validate}')\n",
    "            val_accuracy = correct_validate/val_size \n",
    "            print(f'Epoch: {epoch+1:2} val_Loss:{loss_validate/val_size:10.8f},  validate_Acc:{val_accuracy:4.4f}')\n",
    "            validate_loss.append(loss_validate/val_size)\n",
    "            validate_acc.append(val_accuracy)\n",
    "            # 如果当前模型的准确率优于之前的最佳准确率，则更新最佳模型\n",
    "            #保存当前最优模型参数\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_model = model# 更新最佳模型的参数\n",
    "\n",
    "    \n",
    "    # 保存最后的参数\n",
    "    # torch.save(model, 'final_model_lstm.pt')\n",
    "    # 保存最好的参数\n",
    "    torch.save(best_model, 'best_model_lstm.pt')\n",
    "  \n",
    "    print(f'\\nDuration: {time.time() - start_time:.0f} seconds')\n",
    "    plt.plot(range(epochs), train_loss, color = 'b',label = 'train_loss')\n",
    "    plt.plot(range(epochs), train_acc, color = 'g',label = 'train_acc')\n",
    "    plt.plot(range(epochs), validate_loss, color = 'y',label = 'validate_loss')\n",
    "    plt.plot(range(epochs), validate_acc, color = 'r',label = 'validate_acc')\n",
    "    plt.legend()\n",
    "    plt.show()   #显示 lable \n",
    "    print(\"best_accuracy :\", best_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "# 模型训练\n",
    "model_train(batch_size, epochs, model, optimizer, loss_function, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2960\\1075079830.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('best_model_lstm.pt')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'LSTMclassifier' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 有GPU先用GPU训练\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 得出每一类的分类准确率\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_lstm.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 使用测试集数据进行推断并计算每一类的分类准确率\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1361\u001b[0m             opened_zipfile,\n\u001b[0;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   1363\u001b[0m             pickle_module,\n\u001b[0;32m   1364\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1366\u001b[0m         )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32md:\\ProgramData\\anaconda3\\envs\\py39\\lib\\site-packages\\torch\\serialization.py:1837\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1835\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'LSTMclassifier' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 模型 测试集 验证  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 有GPU先用GPU训练\n",
    "\n",
    "# 得出每一类的分类准确率\n",
    "model = torch.load('best_model_lstm.pt')\n",
    "model = model.to(device)\n",
    "\n",
    "# 使用测试集数据进行推断并计算每一类的分类准确率\n",
    "class_labels = []  # 存储类别标签\n",
    "predicted_labels = []  # 存储预测的标签\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data, test_label in test_loader:\n",
    "        # 将模型设置为评估模式\n",
    "        model.eval()\n",
    "        test_data = test_data.to(device)\n",
    "        test_output = model(test_data)\n",
    "        probabilities = F.softmax(test_output, dim=1)\n",
    "        predicted = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        class_labels.extend(test_label.tolist())\n",
    "        predicted_labels.extend(predicted.tolist())\n",
    "\n",
    "# 混淆矩阵\n",
    "confusion_mat = confusion_matrix(class_labels, predicted_labels)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# 计算每一类的分类准确率\n",
    "report = classification_report(class_labels, predicted_labels, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 绘制混淆矩阵\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 原始标签和自定义标签的映射\u001b[39;00m\n\u001b[0;32m      5\u001b[0m label_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m4\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m }\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# 绘制混淆矩阵\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 原始标签和自定义标签的映射\n",
    "label_mapping = {\n",
    "    0: \"C1\",1: \"C2\",2: \"C3\",3: \"C4\",4: \"C5\",\n",
    "}\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "plt.figure(figsize=(5, 4), dpi=300)\n",
    "sns.heatmap(confusion_mat,  xticklabels=label_mapping.values(), yticklabels=label_mapping.values(),annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix of LSTM-load')\n",
    "plt.savefig(\"lstm_confusion.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
